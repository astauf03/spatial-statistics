freq = term_freq_df$frequency,
min.freq = 10,
max.words = 100,
random.order = FALSE,
rot.per = 0.25,
scale = c(4, 0.5),
colors = brewer.pal(9, "Blues")[4:9]
)
#| message: false
#| warning: false
# Load and preview the sentiment lexicons available in the syuzhet package.
print("NRC LEXICON (FIRST 20 ENTRIES)")
nrc <- syuzhet::get_sentiment_dictionary(dictionary = "nrc")
print(head(nrc, n = 20L))
print("AFINN LEXICON (FIRST 20 ENTRIES)")
afinn <- syuzhet::get_sentiment_dictionary(dictionary = "afinn")
print(head(afinn, n = 20L))
print("BING LEXICON (FIRST 20 ENTRIES)")
bing <- syuzhet::get_sentiment_dictionary(dictionary = "bing")
print(head(bing, n = 20L))
print("SYUZHET LEXICON (FIRST 20 ENTRIES)")
syuzhet_lex <- syuzhet::get_sentiment_dictionary(dictionary = "syuzhet")
print(head(syuzhet_lex, n = 20L))
#| message: false
#| warning: false
# Demonstrate the NRC lexicon with example words.
print("NRC SENTIMENT FOR THE WORD 'profit'")
print(get_nrc_sentiment("profit"))
print("NRC SENTIMENT FOR THE WORD 'loss'")
print(get_nrc_sentiment("loss"))
print("NRC SENTIMENT FOR THE WORD 'crisis'")
print(get_nrc_sentiment("crisis"))
#| message: false
#| warning: false
# Prepare the term frequency data for sentiment analysis.
sentiment_df <- data.frame(
term = term_freq_df$term,
term_frequency = term_freq_df$frequency,
stringsAsFactors = FALSE
)
#| message: false
#| warning: false
# Get NRC sentiment scores for all terms in the corpus.
nrc_sentiment <- get_nrc_sentiment(sentiment_df$term)
# Combine the sentiment scores with the term data.
sentiment_combined <- cbind(sentiment_df, nrc_sentiment)
# Preview the combined data.
print("SENTIMENT DATA (FIRST 10 TERMS)")
print(head(sentiment_combined, 10))
#| message: false
#| warning: false
# Multiply sentiment scores by term frequency to get weighted sentiment.
sentiment_cols <- names(nrc_sentiment)
# Create a copy for weighted calculations.
sentiment_weighted <- sentiment_combined
# Weight each sentiment column by term frequency.
for (col in sentiment_cols) {
sentiment_weighted[[col]] <- sentiment_weighted[[col]] * sentiment_weighted$term_frequency
}
# Preview the weighted data.
print("WEIGHTED SENTIMENT DATA (FIRST 10 TERMS)")
print(head(sentiment_weighted, 10))
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 7
#| fig-dpi: 300
# Calculate total sentiment across all terms.
sentiment_totals <- colSums(sentiment_weighted[, sentiment_cols])
# Create a data frame for plotting.
sentiment_totals_df <- data.frame(
sentiment = names(sentiment_totals),
count = as.numeric(sentiment_totals),
stringsAsFactors = FALSE
)
# Create a bar plot of sentiment totals.
ggplot(sentiment_totals_df, aes(x = reorder(sentiment, -count), y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
scale_fill_brewer(palette = "Set3") +
labs(
title = "Total Sentiment Scores in Reuters Corpus (NRC Lexicon)",
x = "Sentiment Category",
y = "Weighted Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
legend.position = "none"
)
#| message: false
#| warning: false
# Compare sentiment scores across different lexicons.
sentiment_df$syuzhet_score <- as.numeric(get_sentiment(sentiment_df$term, method = "syuzhet"))
sentiment_df$bing_score <- as.numeric(get_sentiment(sentiment_df$term, method = "bing"))
sentiment_df$afinn_score <- as.numeric(get_sentiment(sentiment_df$term, method = "afinn"))
sentiment_df$nrc_score <- as.numeric(get_sentiment(sentiment_df$term, method = "nrc"))
# Preview the multi-lexicon sentiment data.
print("MULTI-LEXICON SENTIMENT SCORES (FIRST 20 TERMS)")
print(head(sentiment_df, 20))
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 10
#| fig-dpi: 300
# Create histograms comparing sentiment distributions across lexicons.
par(mfrow = c(2, 2))
hist(sentiment_df$syuzhet_score, breaks = 30, col = "lightblue", main = "Syuzhet Lexicon Scores", xlab = "Sentiment Score")
hist(sentiment_df$bing_score, breaks = 30, col = "lightgreen", main = "Bing Lexicon Scores", xlab = "Sentiment Score")
hist(sentiment_df$afinn_score, breaks = 30, col = "lightyellow", main = "AFINN Lexicon Scores", xlab = "Sentiment Score")
hist(sentiment_df$nrc_score, breaks = 30, col = "lightpink", main = "NRC Lexicon Scores", xlab = "Sentiment Score")
par(mfrow = c(1, 1))
#| message: false
#| warning: false
# Classify terms as positive, neutral, or negative for each lexicon.
sentiment_classified <- sentiment_df %>%
mutate(
syuzhet_class = sign(syuzhet_score),
bing_class = sign(bing_score),
afinn_class = sign(afinn_score),
nrc_class = sign(nrc_score)
) %>%
mutate(
syuzhet_class = factor(syuzhet_class, levels = c(-1, 0, 1), labels = c("Negative", "Neutral", "Positive")),
bing_class = factor(bing_class, levels = c(-1, 0, 1), labels = c("Negative", "Neutral", "Positive")),
afinn_class = factor(afinn_class, levels = c(-1, 0, 1), labels = c("Negative", "Neutral", "Positive")),
nrc_class = factor(nrc_class, levels = c(-1, 0, 1), labels = c("Negative", "Neutral", "Positive"))
)
# Calculate proportions for each lexicon.
print("SENTIMENT CLASSIFICATION BY LEXICON")
print("Syuzhet Lexicon:")
print(prop.table(table(sentiment_classified$syuzhet_class)))
print("Bing Lexicon:")
print(prop.table(table(sentiment_classified$bing_class)))
print("AFINN Lexicon:")
print(prop.table(table(sentiment_classified$afinn_class)))
print("NRC Lexicon:")
print(prop.table(table(sentiment_classified$nrc_class)))
#| message: false
#| warning: false
# Create a summary table comparing lexicon results.
lexicon_summary <- data.frame(
Lexicon = c("Syuzhet", "Bing", "AFINN", "NRC"),
Negative_Pct = c(
sum(sentiment_classified$syuzhet_class == "Negative", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$bing_class == "Negative", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$afinn_class == "Negative", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$nrc_class == "Negative", na.rm = TRUE) / nrow(sentiment_classified) * 100
),
Neutral_Pct = c(
sum(sentiment_classified$syuzhet_class == "Neutral", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$bing_class == "Neutral", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$afinn_class == "Neutral", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$nrc_class == "Neutral", na.rm = TRUE) / nrow(sentiment_classified) * 100
),
Positive_Pct = c(
sum(sentiment_classified$syuzhet_class == "Positive", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$bing_class == "Positive", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$afinn_class == "Positive", na.rm = TRUE) / nrow(sentiment_classified) * 100,
sum(sentiment_classified$nrc_class == "Positive", na.rm = TRUE) / nrow(sentiment_classified) * 100
)
)
# Round the percentages.
lexicon_summary$Negative_Pct <- round(lexicon_summary$Negative_Pct, 2)
lexicon_summary$Neutral_Pct <- round(lexicon_summary$Neutral_Pct, 2)
lexicon_summary$Positive_Pct <- round(lexicon_summary$Positive_Pct, 2)
# Display as a formatted table.
lexicon_summary %>%
kable(col.names = c("Lexicon", "Negative (%)", "Neutral (%)", "Positive (%)")) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
#| message: false
#| warning: false
# Identify the most positive and most negative terms.
print("TOP 10 MOST POSITIVE TERMS (BY AFINN SCORE)")
top_positive <- sentiment_df %>%
filter(afinn_score > 0) %>%
arrange(desc(afinn_score * term_frequency)) %>%
head(10)
print(top_positive[, c("term", "term_frequency", "afinn_score")])
print("TOP 10 MOST NEGATIVE TERMS (BY AFINN SCORE)")
top_negative <- sentiment_df %>%
filter(afinn_score < 0) %>%
arrange(afinn_score * term_frequency) %>%
head(10)
print(top_negative[, c("term", "term_frequency", "afinn_score")])
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 6
#| fig-dpi: 300
# Calculate document-level sentiment scores.
# Extract text from the cleaned corpus using tm::content() explicitly.
doc_texts <- character(length(reuters_corpus))
for (i in seq_along(reuters_corpus)) {
text_content <- tm::content(reuters_corpus[[i]])
if (length(text_content) > 1) {
doc_texts[i] <- paste(text_content, collapse = " ")
} else if (length(text_content) == 1) {
doc_texts[i] <- text_content
} else {
doc_texts[i] <- ""
}
}
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 6
#| fig-dpi: 300
# Calculate document-level sentiment scores.
# Extract text from the cleaned corpus using tm::content() explicitly.
doc_texts <- character(length(reuters_corpus))
for (i in seq_along(reuters_corpus)) {
text_content <- as.character(reuters_corpus[[i]])
if (length(text_content) > 1) {
doc_texts[i] <- paste(text_content, collapse = " ")
} else if (length(text_content) == 1) {
doc_texts[i] <- text_content
} else {
doc_texts[i] <- ""
}
}
# Calculate sentiment for each document using different methods.
doc_sentiment <- data.frame(
doc_id = seq_along(doc_texts),
syuzhet = sapply(doc_texts, function(x) sum(get_sentiment(x, method = "syuzhet"))),
bing = sapply(doc_texts, function(x) sum(get_sentiment(x, method = "bing"))),
afinn = sapply(doc_texts, function(x) sum(get_sentiment(x, method = "afinn"))),
nrc = sapply(doc_texts, function(x) sum(get_sentiment(x, method = "nrc")))
)
# Preview document sentiment.
print("DOCUMENT-LEVEL SENTIMENT SCORES (FIRST 10 DOCUMENTS)")
print(head(doc_sentiment, 10))
# Plot document sentiment distribution.
ggplot(doc_sentiment, aes(x = doc_id, y = afinn)) +
geom_bar(stat = "identity", fill = ifelse(doc_sentiment$afinn >= 0, "steelblue", "coral")) +
geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
labs(
title = "Document-Level Sentiment Scores (AFINN Lexicon)",
x = "Document ID",
y = "Sentiment Score"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold")
)
#| message: false
#| warning: false
# To make clustering feasible, we need to remove sparse (infrequently occurring) terms.
# This keeps only terms that appear in at least a certain percentage of documents.
# Using 0.99 sparsity means we keep terms that appear in at least 1% of documents.
dtm_cluster <- removeSparseTerms(dtm, 0.99)
# Convert the cleaned DTM to a matrix.
m_cluster <- as.matrix(dtm_cluster)
print(paste("Cleaned Matrix Dimensions:", nrow(m_cluster), "documents x", ncol(m_cluster), "terms"))
#| message: false
#| warning: false
#| cache: true
#| fig-width: 10
#| fig-height: 8
#| fig-dpi: 300
# Prepare the data subset for NbClust analysis.
# Use a subset of 50 documents to make computation faster.
m_subset <- m_cluster[1:50, ]
# Remove columns that have zero variance (constant values cause issues for clustering).
col_variances <- apply(m_subset, 2, var)
m_subset_clean <- m_subset[, col_variances > 0]
print(paste("Subset dimensions after removing zero-variance columns:", nrow(m_subset_clean), "x", ncol(m_subset_clean)))
# Use the NbClust package to determine the optimal number of clusters (k).
set.seed(42)
optimal_clusters <- NbClust(
data = m_subset_clean,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans",
index = "all"
)
#| message: false
#| warning: false
# To make clustering feasible, we need to remove sparse (infrequently occurring) terms.
# This keeps only terms that appear in at least a certain percentage of documents.
# Using 0.99 sparsity means we keep terms that appear in at least 1% of documents.
dtm_cluster <- removeSparseTerms(dtm, 0.99)
# Convert the cleaned DTM to a matrix.
m_cluster <- as.matrix(dtm_cluster)
print(paste("Cleaned Matrix Dimensions:", nrow(m_cluster), "documents x", ncol(m_cluster), "terms"))
#| message: false
#| warning: false
#| cache: true
#| fig-width: 10
#| fig-height: 6
#| fig-dpi: 300
# Prepare the data for clustering analysis.
# For sparse text data, NbClust often fails due to matrix singularity issues.
# Instead, we use the elbow method (within-cluster sum of squares) to find optimal k.
# Remove columns that have zero variance (constant values cause issues for clustering).
col_variances <- apply(m_cluster, 2, var)
m_cluster_clean <- m_cluster[, col_variances > 0]
print(paste("Matrix dimensions after removing zero-variance columns:", nrow(m_cluster_clean), "x", ncol(m_cluster_clean)))
# For very sparse matrices, we can also remove very rare terms to improve clustering.
# Keep only terms that appear in at least 5% of documents.
term_doc_freq <- colSums(m_cluster_clean > 0) / nrow(m_cluster_clean)
m_cluster_reduced <- m_cluster_clean[, term_doc_freq >= 0.05]
print(paste("Matrix dimensions after filtering rare terms:", nrow(m_cluster_reduced), "x", ncol(m_cluster_reduced)))
# Use the Elbow Method to determine optimal number of clusters.
# Calculate within-cluster sum of squares for k = 1 to 10.
set.seed(42)
wss <- numeric(10)
for (k in 1:10) {
kmeans_temp <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
wss[k] <- kmeans_temp$tot.withinss
}
# Create a data frame for plotting.
elbow_df <- data.frame(
k = 1:10,
wss = wss
)
# Plot the elbow curve.
ggplot(elbow_df, aes(x = k, y = wss)) +
geom_line(color = "steelblue", size = 1) +
geom_point(color = "steelblue", size = 3) +
scale_x_continuous(breaks = 1:10) +
labs(
title = "Elbow Method for Optimal Number of Clusters",
x = "Number of Clusters (k)",
y = "Within-Cluster Sum of Squares"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold")
)
# Print the WSS values.
print("WITHIN-CLUSTER SUM OF SQUARES BY K")
print(elbow_df)
#| message: false
#| warning: false
# Based on the elbow plot, select the optimal k.
# Typically, we look for the "elbow" where the rate of decrease sharply changes.
# For text data, k = 3 or k = 4 is often a good choice.
k <- 4
# Run k-means on the reduced matrix (with filtered rare terms).
set.seed(42)
kmeans_result <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
# Create a data frame with document IDs and their cluster assignments.
cluster_assignments <- data.frame(
doc_id = 1:nrow(m_cluster_reduced),
cluster = kmeans_result$cluster
)
# Display the size of each cluster.
print("CLUSTER SIZES")
print(table(kmeans_result$cluster))
# Display clustering quality metrics.
print(paste("Total within-cluster sum of squares:", round(kmeans_result$tot.withinss, 2)))
print(paste("Between-cluster sum of squares:", round(kmeans_result$betweenss, 2)))
print(paste("BSS/TSS ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 2), "%"))
#| message: false
#| warning: false
# Get the cluster centers (each row is a cluster, each column is a term).
cluster_centers <- kmeans_result$centers
# Create a data frame of cluster centers with term names.
cluster_centers_df <- as.data.frame(cluster_centers)
colnames(cluster_centers_df) <- colnames(m_cluster_reduced)
# Define a function to get top terms for each cluster.
get_top_terms <- function(cluster_center, term_names, n = 10) {
# Sort terms by their center value (importance in the cluster).
sorted_indices <- order(cluster_center, decreasing = TRUE)
top_indices <- sorted_indices[1:n]
result <- data.frame(
Term = term_names[top_indices],
Prominence = round(cluster_center[top_indices], 3)
)
return(result)
}
# Get top 10 terms for each cluster.
term_names <- colnames(cluster_centers_df)
for (i in 1:k) {
print(paste("TOP 10 TERMS FOR CLUSTER", i))
top_terms <- get_top_terms(as.numeric(cluster_centers_df[i, ]), term_names, n = 10)
print(top_terms)
cat("\n")
}
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6
#| fig-dpi: 300
# Create a bar plot showing cluster sizes.
cluster_size_df <- data.frame(
Cluster = factor(1:k),
Size = as.numeric(table(kmeans_result$cluster))
)
ggplot(cluster_size_df, aes(x = Cluster, y = Size, fill = Cluster)) +
geom_bar(stat = "identity") +
geom_text(aes(label = Size), vjust = -0.5, size = 4) +
scale_fill_brewer(palette = "Set2") +
labs(
title = "Document Distribution Across K-Means Clusters",
x = "Cluster",
y = "Number of Documents"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
legend.position = "none"
)
#| message: false
#| warning: false
# Calculate overall corpus sentiment statistics.
print("OVERALL CORPUS SENTIMENT SUMMARY")
print(paste("Mean AFINN sentiment:", round(mean(doc_sentiment$afinn), 3)))
print(paste("Median AFINN sentiment:", round(median(doc_sentiment$afinn), 3)))
print(paste("Standard deviation:", round(sd(doc_sentiment$afinn), 3)))
print(paste("Documents with positive sentiment:", sum(doc_sentiment$afinn > 0)))
print(paste("Documents with negative sentiment:", sum(doc_sentiment$afinn < 0)))
print(paste("Documents with neutral sentiment:", sum(doc_sentiment$afinn == 0)))
#| message: false
#| warning: false
# Calculate overall corpus sentiment statistics.
print("OVERALL CORPUS SENTIMENT SUMMARY")
print(paste("Mean AFINN Sentiment:", round(mean(doc_sentiment$afinn), 3)))
print(paste("Median AFINN Sentiment:", round(median(doc_sentiment$afinn), 3)))
print(paste("Standard Deviation:", round(sd(doc_sentiment$afinn), 3)))
print(paste("Documents with Positive Sentiment:", sum(doc_sentiment$afinn > 0)))
print(paste("Documents with Negative Sentiment:", sum(doc_sentiment$afinn < 0)))
print(paste("Documents with Neutral Sentiment:", sum(doc_sentiment$afinn == 0)))
#| message: false
#| warning: false
# Based on the elbow plot, select the optimal k.
# Typically, we look for the "elbow" where the rate of decrease sharply changes.
# For text data, k = 3 or k = 4 is often a good choice.
k <- 4
# Run k-means on the reduced matrix (with filtered rare terms).
set.seed(42)
kmeans_result <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
# Create a data frame with document IDs and their cluster assignments.
cluster_assignments <- data.frame(
doc_id = 1:nrow(m_cluster_reduced),
cluster = kmeans_result$cluster
)
# Display the size of each cluster.
print("CLUSTER SIZES")
print(table(kmeans_result$cluster))
# Display clustering quality metrics.
print(paste("\nTotal Within-Cluster Sum of Squares:", round(kmeans_result$tot.withinss, 2)))
print(paste("Between-Cluster Sum of Squares:", round(kmeans_result$betweenss, 2)))
print(paste("BSS/TSS Ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 2), "%"))
#| message: false
#| warning: false
# Based on the elbow plot, select the optimal k.
# Typically, we look for the "elbow" where the rate of decrease sharply changes.
# For text data, k = 3 or k = 4 is often a good choice.
k <- 4
# Run k-means on the reduced matrix (with filtered rare terms).
set.seed(42)
kmeans_result <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
# Create a data frame with document IDs and their cluster assignments.
cluster_assignments <- data.frame(
doc_id = 1:nrow(m_cluster_reduced),
cluster = kmeans_result$cluster
)
# Display the size of each cluster.
print("CLUSTER SIZES")
print(table(kmeans_result$cluster))
# Display clustering quality metrics.
print("")
print(paste("Total Within-Cluster Sum of Squares:", round(kmeans_result$tot.withinss, 2)))
print(paste("Between-Cluster Sum of Squares:", round(kmeans_result$betweenss, 2)))
print(paste("BSS/TSS Ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 2), "%"))
#| message: false
#| warning: false
# Based on the elbow plot, select the optimal k.
# Typically, we look for the "elbow" where the rate of decrease sharply changes.
# For text data, k = 3 or k = 4 is often a good choice.
k <- 4
# Run k-means on the reduced matrix (with filtered rare terms).
set.seed(42)
kmeans_result <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
# Create a data frame with document IDs and their cluster assignments.
cluster_assignments <- data.frame(
doc_id = 1:nrow(m_cluster_reduced),
cluster = kmeans_result$cluster
)
# Display the size of each cluster.
print("CLUSTER SIZES")
print(table(kmeans_result$cluster))
# Display clustering quality metrics.
cat("")
print(paste("Total Within-Cluster Sum of Squares:", round(kmeans_result$tot.withinss, 2)))
print(paste("Between-Cluster Sum of Squares:", round(kmeans_result$betweenss, 2)))
print(paste("BSS/TSS Ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 2), "%"))
#| message: false
#| warning: false
# Based on the elbow plot, select the optimal k.
# Typically, we look for the "elbow" where the rate of decrease sharply changes.
# For text data, k = 3 or k = 4 is often a good choice.
k <- 4
# Run k-means on the reduced matrix (with filtered rare terms).
set.seed(42)
kmeans_result <- kmeans(m_cluster_reduced, centers = k, nstart = 25, iter.max = 100)
# Create a data frame with document IDs and their cluster assignments.
cluster_assignments <- data.frame(
doc_id = 1:nrow(m_cluster_reduced),
cluster = kmeans_result$cluster
)
# Display the size of each cluster.
print("CLUSTER SIZES")
print(table(kmeans_result$cluster))
# Display clustering quality metrics.
cat("\n")
print(paste("Total Within-Cluster Sum of Squares:", round(kmeans_result$tot.withinss, 2)))
print(paste("Between-Cluster Sum of Squares:", round(kmeans_result$betweenss, 2)))
print(paste("BSS/TSS Ratio:", round(kmeans_result$betweenss / kmeans_result$totss * 100, 2), "%"))
